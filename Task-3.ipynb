{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discuss the implications and advantages of each scenario and explain your rationale as to how the model should be trained given the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. If the entire network should be frozen.\n",
    "\n",
    "If the entire network is frozen, then even if we train the model, none of the parameter weights will get updated and they will remain the same.\n",
    "\n",
    "Possible implications would be, regarding our specific model, that it will just extract the features which is not really useful when creating multi-tasking models.\n",
    "Due to this, our classification and sentiment heads will not be able to learn anything meaningful and will not understand specific use cases.\n",
    "\n",
    "Doing this might be advantageous when our underlying transformer model is already performing really well on desired tasks.\n",
    "\n",
    "### 2. If only the transformer backbone should be frozen.\n",
    "\n",
    "This is a common approach when working with Large Language models to make them do specific tasks without spending much and without allocating a lot of computational resources.\n",
    "\n",
    "This approach could harm us if proper research has not been done. For example, the underlying models pretrained weights might not be optimal for our use-case.\n",
    "To be more specific, if we freeze the transformer pretrained to write SQL code and we want to train the heads to write Machine Learning code. This method will not give us good results.\n",
    "For good results, transformer pretrained to write python code should be used.\n",
    "\n",
    "Some advantages of this could be computational efficiency and faster training if working with a relatively smaller dataset (Transformer is frozen but our linear layers from the two heads will get tarined.)\n",
    "In this case, on a very high level, our classification and sentiment heads will act similarly as a LoRA and undergo efficient fine-tuning on a smaller dataset \n",
    "(I just mentioned LoRA as an example. Please note that they will act similar but they are not the same.)\n",
    "\n",
    "### 3. If only one of the task-specific heads (either for Task A or Task B) should be frozen.\n",
    "\n",
    "This method can be disadvantageous for the overall quality of the model. For example, the unfrozen head might perform really well after training while the frozen head will not, bringing down the overall quality.\n",
    "\n",
    "\n",
    "This approach can help us in fine-tuning the unfrozen head efficiently as freezing one whole head will decrease the trainable parameters to a significant extent. I think we should use this method if our\n",
    "use-case demands us to prioritize one task among the multi-tasks. Other scenario when this would be usefull would be when we have imbalanced datasets and hence we freeze the head with insufficient data.\n",
    "It is also useful to freeze one head which is performing well just on the pretrained weights just to save resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider a scenario where transfer learning can be beneficial. Explain how you would approach the transfer learning process, including:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we have limited data or we lack enormous computation, transfer learning is very beneficial. \n",
    "I will consider a scenario where I have to create an app leveraging LLM which users can use to seek therapy or they just want to try counselling to know more about it.\n",
    "\n",
    "### 1. The choice of a pre-trained model.\n",
    "\n",
    "My choice of pre-trained model would depend on two things majorly - size and use case.\n",
    "\n",
    "1. Firstly, I am in the early stages of the app development. I lack resources as well as audience. Hence I would choose a smaller model, probably a 7B parameter model.\n",
    "   A 7B parameter would beneficial in my early stages of app development and I would be able to efficiently fine-tune it to serve meaningful counselling to a smaller audience.\n",
    "\n",
    "2. Secondly, while talking with a therapist, either you ask him questions and they answer them, or they ask you questions and you clarify. This is a very chat based scenario and hence\n",
    "   I will go forward with a model which is optimized for chat. To be more specific, I will move forward with a Mistral-7B-Tnstruct Model.\n",
    "\n",
    "### 2. The layers you would freeze/unfreeze.\n",
    "\n",
    "Now, while fine-tuning the pretrained model for our task, I will freeze all the pretrained layers of the transformer and only train the additional layers I have added for transfer learning.\n",
    "This would minimize the computation requirements. If results are satisfactory, I would go forward to the next steps of development.\n",
    "\n",
    "But, if doing transfer learning this way does not render good results, I would unfreeze the pretrained model's later layers and redo the transfer learning. The number later layers to unfreeze is\n",
    "much similar to a hyperparameter and it would take me some experiments to reach the optimal resource - performance tradeoff.\n",
    "\n",
    "### 3. The rationale behind these choices.\n",
    "\n",
    "The rational to choose the Mistral-7B-Instruct model lies behind the fact that it has lesser parameters and hence is efficient, it has ample resources and documentation which will help me develop \n",
    "my LLM smoothly and that the instruct type of model is easily available.\n",
    "\n",
    "The rational behind the freezing all layers initially is that just in case if fine-tuning the LLM that way results in a well-performing model, I would save a lot of time and resources. If it did\n",
    "not perform well whit all layers frozen, then, freezing the early layers and unfreezing the later layers will help the model to not overfit and generalize to the dataset. This is because the early\n",
    "layers contain core information about the embeddings or shapes etc and hence are transferable to general data. The later layers include high level information about the features of the task and can adapt to new tasks.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
