{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrt0quakU7Qf"
      },
      "source": [
        "# Sentence Transformer Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KohomtfKU7Qg",
        "outputId": "f2889032-6592-406c-cb60-ba38df1b9be7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/vipulsarode/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/Users/vipulsarode/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zM6bmCJIU7Qh"
      },
      "outputs": [],
      "source": [
        "# Defining a SentenceTransformer Class which inherits torch.nn.Module and uses BERT model and takes sentences as input to give fixed-size shared embeddings\n",
        "\n",
        "class SentenceTransformer(nn.Module):\n",
        "    def __init__(self, model_name='bert-base-uncased'):\n",
        "        super().__init__()\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name) # Importing BertTokenizer to tokenize sentences\n",
        "        self.bert = BertModel.from_pretrained(model_name) # Importing BertModel to extract contextualized embeddings from sentences\n",
        "\n",
        "\n",
        "    def forward(self, sentences):\n",
        "        tokens = self.tokenizer(sentences, padding = True, truncation = True, return_tensors = 'pt') # Tokenizing all the sentences\n",
        "        outputs = self.bert(input_ids=tokens['input_ids'], token_type_ids=tokens['token_type_ids'], attention_mask=tokens['attention_mask']) # Using BertModel to extract embeddings\n",
        "        embeddings = outputs.last_hidden_state.mean(dim = 1) # Performing pooling i.e. averaging the last hidden state embeddings over the sequence length to get fixed size embeddings\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsPTlsmLU7Qi"
      },
      "outputs": [],
      "source": [
        "# Creating a list of sentences with variable length to check if we get the embeddings of fixed-size\n",
        "\n",
        "sentences = ['I love Pizza',\n",
        "            'I went to the zoo and saw a tiger',\n",
        "            'After all, you are my wonderwall']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daQWQzm5U7Qi",
        "outputId": "843a4703-9b9b-4381-9f01-92c2c76ad5ec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/vipulsarode/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Calling out the model to perform preprocessing of the sentences\n",
        "sentence_transformer = SentenceTransformer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxPwZNhXU7Qi"
      },
      "outputs": [],
      "source": [
        "# giving the list of sentences to the model as an input\n",
        "embeds = sentence_transformer(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBx9inyWU7Qi",
        "outputId": "f11c3d62-77f8-4bd5-e9a0-61b39107ffbb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.2334,  0.1736,  0.1612,  ...,  0.2010,  0.1730, -0.0193],\n",
              "        [ 0.0689, -0.3667, -0.3938,  ..., -0.4283,  0.0947,  0.0498],\n",
              "        [ 0.3152,  0.0161,  0.2936,  ..., -0.0361,  0.0472, -0.1615]],\n",
              "       grad_fn=<MeanBackward1>)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Preview of the embeddings\n",
        "embeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nvxbv80QU7Qi",
        "outputId": "c5d50362-75da-45d4-fdbb-6f41c09e6bce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shape of the embeddings are torch.Size([3, 768])\n",
            "\n",
            "Checking the length of embeddings to make sure it is the same for all elements\n",
            "\n",
            "Length of Sentence one torch.Size([768])\n",
            "Length of Sentence two torch.Size([768])\n",
            "Length of Sentence three torch.Size([768])\n"
          ]
        }
      ],
      "source": [
        "# CHecking the Embeddings shape, and the shape of individual embedding s of each sentence to make sure they are of same length.\n",
        "\n",
        "print('The shape of the embeddings are', embeds.shape)\n",
        "print('\\nChecking the length of embeddings to make sure it is the same for all elements\\n')\n",
        "print('Length of Sentence one', embeds[0].shape)\n",
        "print('Length of Sentence two', embeds[1].shape)\n",
        "print('Length of Sentence three', embeds[2].shape)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}